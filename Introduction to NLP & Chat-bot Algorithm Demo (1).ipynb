{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                             \n",
    "                                             \n",
    "                                             Company: I2Decisions\n",
    "                                               Author : Melvin Roy V\n",
    "                                                 Date   : 1/4/2017\n",
    "                                          Topic  : Sentence Matching Algorithm\n",
    "                                          \n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "NLP Basics\n",
    "\n",
    "What is Natural Language Processing?\n",
    "\n",
    "Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve: natural language understanding, enabling computers to derive meaning from human\n",
    "or natural language input; and others involve natural language generation.\n",
    "\n",
    "Applications:\n",
    "\n",
    "Spam Detection, Part of-speech tagging ,Named entity recognition(NER), Sentiment Analysis, Parsing, Machine Translation, \n",
    "Information Extraction, Question Answering, Paraphrase, Summarization, Dialog\n",
    "\n",
    "Resources: \n",
    "1. https://www.youtube.com/watch?v=nfoudtpBV68 -Awesome videos by Professor Dan Jurafsky & Chris Manning \n",
    "2. https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "3. Natural Language Processing With Python-Steven Bird, Ewan Klein, Edward Loper\n",
    "4. Python 3 Text Processing with NLTK 3 Cookbook Jacob perkins\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How to Contact Chamberlain Technical Support?']\n",
      "['How', 'to', 'Contact', 'Chamberlain', 'Technical', 'Support', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NLP Basic Terminologies:\n",
    "\n",
    "Tokenization : Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or \n",
    "other meaningful elements called tokens.\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Tokenization\n",
    "\n",
    "'''\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence = \"How to Contact Chamberlain Technical Support?\"\n",
    "\n",
    "print(sent_tokenize(sentence))\n",
    "print(word_tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'to', 'Contact', 'Chamberlain', 'Technical', 'Support', '?']\n",
      "['How', 'Contact', 'Chamberlain', 'Technical', 'Support', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "NLP Basics Terminologies:\n",
    "\n",
    "Stopwords : Stop words are words which are filtered out before or after processing of natural language data (text)\n",
    "\n",
    "Source : https://en.wikipedia.org/wiki/Stop_words\n",
    "\n",
    "'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"How to Contact Chamberlain Technical Support?\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact\n",
      "contact\n",
      "contact\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "NLP Basics Terminologies:\n",
    "\n",
    "Stemming : Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root \n",
    "form—generally a written word form.\n",
    "\n",
    "Source : https://en.wikipedia.org/wiki/Stemming\n",
    "\n",
    "'''\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"contact\",\"contacted\",\"contacting\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let's start.\n",
    "\n",
    "Today I am going to teach you how sentence matching alogorithm is working in the backend. I am going to compare Api.ai algorithm with our python code.\n",
    "\n",
    "Consider you have a sentence of interest and would like to match it with all the sentences within the corpus. \n",
    "\n",
    "How will you go about it?\n",
    "\n",
    "What are the issued faced?\n",
    "    a.Capitalization\n",
    "    b.Punctuation\n",
    "    c.White-spacing\n",
    "\n",
    "Lets look at the example below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exact Matching\n",
    "\n",
    "target_sentence = \"How to Contact Chamberlain Technical Support?\"\n",
    "\n",
    "sentences = [\"How to Contact Chamberlain Technical Support?\",\n",
    "             \"how to  contact Chamberlain technical support?\",\n",
    "             \"Contact Chamberlain Technical Support?\",\n",
    "             \"Contacting Chamberlain Technical Support?\",\n",
    "             \"Chamberlain Technical Support\",\n",
    "             \"Technical Support\",\n",
    "             \"Tech Support\",\n",
    "             \"Call technical support\",\n",
    "             \"Chamberlain Technical Support\",]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'How to Contact Chamberlain Technical Support?')\n",
      "(False, 'how to  contact Chamberlain technical support?')\n",
      "(False, 'Contact Chamberlain Technical Support?')\n",
      "(False, 'Contacting Chamberlain Technical Support?')\n",
      "(False, 'Chamberlain Technical Support')\n",
      "(False, 'Technical Support')\n",
      "(False, 'Tech Support')\n",
      "(False, 'Call technical support')\n",
      "(False, 'Chamberlain Technical Support')\n"
     ]
    }
   ],
   "source": [
    "# Exact Match\n",
    "def exact_match(a, b):\n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    return (a == b)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(exact_match(target_sentence, sentence), sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the solution?\n",
    "\n",
    "Fuzzy Sentence Matching \n",
    "\n",
    "Fuzzy logic is an approach to computing based on \"degrees of truth\" rather than the usual \"true or false\" (1 or 0) Boolean logic on which the modern computer is based. \n",
    "\n",
    "Source:whatis.techtarget.com/definition/fuzzy-logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types:\n",
    "    \n",
    "1. Exact case-insensitive token match\n",
    "2. Exact case-insensitive stem matching after stopword removal\n",
    "3. Exact case-insensitive stem matching after stopword removal and stemming\n",
    "4. Exact Case-Insensitive Token set similarity after stopword removal(Jaccard similarity index)\n",
    "5. Exact Case-Insensitive Token set similarity after stopword removal and stemming (Jaccard similarity index)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'How to Contact Chamberlain Technical Support?')\n",
      "(True, 'how to  contact Chamberlain technical support?')\n",
      "(False, 'Contact Chamberlain Technical Support?')\n",
      "(False, 'Contacting Chamberlain Technical Support?')\n",
      "(False, 'Chamberlain Technical Support')\n",
      "(False, 'Technical Support')\n",
      "(False, 'Tech Support')\n",
      "(False, 'Call technical support')\n",
      "(False, 'Chamberlain Technical Support')\n"
     ]
    }
   ],
   "source": [
    "# Exact Case-Insensitive Token Match \n",
    "import nltk\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "def is_ci_token_match(a, b):\n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a)]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b)]\n",
    "\n",
    "    return (tokens_a == tokens_b)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(is_ci_token_match(target_sentence, sentence), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'How to Contact Chamberlain Technical Support?')\n",
      "(True, 'how to  contact Chamberlain technical support?')\n",
      "(True, 'Contact Chamberlain Technical Support?')\n",
      "(False, 'Contacting Chamberlain Technical Support?')\n",
      "(False, 'Chamberlain Technical Support')\n",
      "(False, 'Technical Support')\n",
      "(False, 'Tech Support')\n",
      "(False, 'Call technical support')\n",
      "(False, 'Chamberlain Technical Support')\n"
     ]
    }
   ],
   "source": [
    "# Exact Case-Insensitive Token Match After Removing Stopwords\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "def is_ci_token_stopword_match(a, b):\n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    \n",
    "    return (tokens_a == tokens_b)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(is_ci_token_stopword_match(target_sentence, sentence), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'How to Contact Chamberlain Technical Support?')\n",
      "(True, 'how to  contact Chamberlain technical support?')\n",
      "(True, 'Contact Chamberlain Technical Support?')\n",
      "(True, 'Contacting Chamberlain Technical Support?')\n",
      "(False, 'Chamberlain Technical Support')\n",
      "(False, 'Technical Support')\n",
      "(False, 'Tech Support')\n",
      "(False, 'Call technical support')\n",
      "(False, 'Chamberlain Technical Support')\n"
     ]
    }
   ],
   "source": [
    "# Exact Case-Insensitive Token Match After Removing Stopwords & Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "def is_ci_token_stopword_stem_match(a, b):\n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    stems_a = [stemmer.stem(token) for token in tokens_a]\n",
    "    stems_b = [stemmer.stem(token) for token in tokens_b]\n",
    "\n",
    "    return (stems_a == stems_b)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(is_ci_token_stopword_stem_match(target_sentence, sentence), sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                \n",
    "                                                #Approximate Sentence Matching\n",
    "\n",
    "\n",
    "In the above three methods we transformed or removed the elements from input sequences, then compare it with the output sequence for exact match. \n",
    "\n",
    "Now we are going to relax our rules little bit. We are going to do a set similarity match instead of a exact sequence matching. For set similarity measure we can use Jaccard similarity index, which is based on the simple set operations union and intersection.\n",
    "\n",
    "Formula:\n",
    "    \n",
    "    \n",
    "    J(A,B) = |A n B| / |A u B| , 0<=J(A,B)<=1\n",
    "    \n",
    "\n",
    "Example:\n",
    "\n",
    "    Sentence 1: How to Contact Chamberlain Technical Support?\n",
    "    Sentence 2: Contact Chamberlain Technical Support\n",
    "    \n",
    "    J(A n B) = 4\n",
    "    J(A u B) = 6\n",
    "    \n",
    "    J(S1,S2) = 4/6 = 0.66\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'How to Contact Chamberlain Technical Support?')\n",
      "(True, 'how to  contact Chamberlain technical support?')\n",
      "(True, 'Contact Chamberlain Technical Support?')\n",
      "(True, 'Contacting Chamberlain Technical Support?')\n",
      "(True, 'Chamberlain Technical Support')\n",
      "(True, 'Technical Support')\n",
      "(False, 'Tech Support')\n",
      "(False, 'Call technical support')\n",
      "(True, 'Chamberlain Technical Support')\n"
     ]
    }
   ],
   "source": [
    "# Exact Case-Insensitive Token set similarity after stopword removal using Jaccard similarity index\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "\n",
    "# Create tokenizer and stemmer\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "def is_ci_token_stopword_set_match(a, b, threshold=0.5):\n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    ratio = len(set(tokens_a).intersection(tokens_b)) / float(len(set(tokens_a).union(tokens_b)))\n",
    "    return (ratio >= threshold)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(is_ci_token_stopword_set_match(target_sentence, sentence), sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'How to Contact Chamberlain Technical Support?')\n",
      "(True, 'how to  contact Chamberlain technical support?')\n",
      "(True, 'Contact Chamberlain Technical Support?')\n",
      "(True, 'Contacting Chamberlain Technical Support?')\n",
      "(True, 'Chamberlain Technical Support')\n",
      "(True, 'Technical Support')\n",
      "(False, 'Tech Support')\n",
      "(False, 'Call technical support')\n",
      "(True, 'Chamberlain Technical Support')\n"
     ]
    }
   ],
   "source": [
    "# Exact Case-Insensitive Token set similarity after stopword removal and stemming using Jaccard similarity index\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "\n",
    "# Create tokenizer and stemmer\n",
    "tokenizer =nltk.tokenize.TreebankWordTokenizer()\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "def is_ci_stem_stopword_set_match(a, b, threshold=0.5):\n",
    "    \"\"\"Check if a and b are matches.\"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) \\\n",
    "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
    "    stems_a = [stemmer.stem(token) for token in tokens_a]\n",
    "    stems_b = [stemmer.stem(token) for token in tokens_b]\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    ratio = len(set(stems_a).intersection(stems_b)) / float(len(set(stems_a).union(stems_b)))\n",
    "    return (ratio >= threshold)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(is_ci_token_stopword_set_match(target_sentence, sentence), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                                                        \n",
    "    \n",
    "                                                        Thank you\n",
    "                                                \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
